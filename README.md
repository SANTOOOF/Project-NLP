# üß¨ Biomedical Named Entity Recognition (Bio-NER) on the GENIA Corpus

## üìú Description du Projet

Ce projet a pour objectif de concevoir un syst√®me capable d'identifier et de classifier automatiquement les entit√©s biologiques (Prot√©ines, ADN, ARN, etc.) dans les r√©sum√©s d'articles scientifiques[cite: 28]. [cite_start]Nous avons impl√©ment√© un pipeline robuste en Python, combinant des techniques de Machine Learning classiques avec un *Feature Engineering* avanc√© et le d√©codage de Viterbi pour garantir la coh√©rence des pr√©dictions[cite: 29, 31].

---

## üéØ Objectif Principal

D√©veloppement d'un syst√®me de Bio-NER sur le corpus GENIA (Version 3.02), la r√©f√©rence standard pour l'extraction d'informations dans le domaine biom√©dical[cite: 28, 35].

## üìä Jeu de Donn√©es et Classes Cibles

* **Corpus utilis√©** : GENIA Corpus (Version 3.02), issu d'extraits d'articles de la base de donn√©es MEDLINE[cite: 35, 37].
* **Volume** : Environ 23,793 entit√©s nomm√©es annot√©es manuellement[cite: 38].
* **Format** : XML, n√©cessitant un parsing pr√©cis[cite: 39].
* **5 Classes Cibles** (simplifi√©es √† partir de 36 classes fines)[cite: 41]:
    1.  **Protein** [cite: 42]
    2.  **DNA** [cite: 43]
    3.  **RNA** [cite: 44]
    4.  **Cell Line** [cite: 45]
    5.  **Cell Type** [cite: 46]

---

## üõ†Ô∏è M√©thodologie et Impl√©mentation

### 1. Pr√©-traitement et Encodage [cite: 50]

* **Encodage** : Sch√©ma standard **BIO** (Begin, Inside, Outside)[cite: 52].
* **Class Splitting** : Division de la classe majoritaire "O" (Outside) en sous-classes bas√©es sur les tags Part-of-Speech (e.g., O-NN, O-VB) pour am√©liorer la distinction du contexte linguistique[cite: 53, 54].

### 2. Feature Engineering [cite: 55]

Un vecteur de caract√©ristiques a √©t√© g√©n√©r√© pour chaque mot, incluant :

* **Contexte Morphologique** : Le mot, ses pr√©fixes (2 et 3 lettres), suffixes, pr√©sence de majuscules ou de chiffres[cite: 57].
* **Fen√™tre Glissante (Sliding Window)** : Caract√©ristiques des 2 mots pr√©c√©dents et suivants[cite: 58].
* **Word Cache** : Indication binaire si le mot a d√©j√† √©t√© vu comme une entit√©[cite: 59].
* **HMM States** : √âtats d'un Mod√®le de Markov Cach√© (HMM) non supervis√© pour capturer des structures latentes[cite: 60].

### 3. Mod√®les et Post-traitement [cite: 61]

* **Mod√®les Entra√Æn√©s** : Random Forest, SVM, XGBoost[cite: 62].
* **Post-traitement Crucial** : Impl√©mentation de l'**algorithme de Viterbi**[cite: 63].
    * **R√¥le** : Utilise une matrice de transition pour corriger les erreurs de s√©quence ill√©gales (e.g., un tag `I-Protein` isol√©) et garantir la validit√© des s√©quences BIO[cite: 63, 166].

---

## üöÄ R√©sultats Cl√©s

### 1. Comparaison des Mod√®les

Les mod√®les lin√©aires ont domin√© le classement, sugg√©rant l'efficacit√© du *feature engineering* √† rendre le probl√®me lin√©airement s√©parable dans un espace de haute dimension[cite: 71].

| Mod√®le | Rang | Pr√©cision (Accuracy) | F1-Score (Pond√©r√©) |
| :--- | :---: | :---: | :---: |
| **Linear SVM** | **1** | [cite_start]**88.40%** [cite: 70] | [cite_start]**0.88** [cite: 70] |
| Logistic Regression | 2 | [cite_start]88.20% [cite: 70] | [cite_start]0.88 [cite: 70] |
| SVM (RBF Kernel) | 3 | [cite_start]85.97% [cite: 70] | [cite_start]0.85 [cite: 70] |
| Random Forest | 4 | [cite_start]85.22% [cite: 70] | [cite_start]0.84 [cite: 70] |

* **Meilleure Performance** : Le **Linear SVM** a atteint une pr√©cision de $\mathbf{88.40\%}$ et un F1-Score pond√©r√© de **0.88**[cite: 70].

<img width="1189" height="590" alt="output" src="https://github.com/user-attachments/assets/5042d500-cd31-424b-a7e4-276945aebdb4" />


<img width="797" height="701" alt="output1" src="https://github.com/user-attachments/assets/aa01e3f8-3f93-4de0-a5db-12574791f7dd" />

### 2. Analyse des Erreurs

* Les classes **Protein** et **Cell Type** sont les mieux d√©tect√©es[cite: 91].
* La classe **DNA** est souvent confondue avec les prot√©ines en raison de contextes syntaxiques similaires[cite: 91].

### 3. Impact Qualitatif de Viterbi

L'algorithme de Viterbi s'est av√©r√© crucial pour la coh√©rence structurelle[cite: 171].

> **Exemple de correction** : Le mod√®le Standard Random Forest a pr√©dit un tag `I-protein` isol√© pour le mot "5-lipoxygenase". [cite_start]Le d√©codeur de Viterbi, en appliquant les r√®gles de transition BIO, a corrig√© cette erreur en for√ßant le tag `B-protein` (D√©but d'entit√©)[cite: 165, 166].

---

## üí° Conclusion et Perspectives

Ce projet a valid√© l'efficacit√© d'une approche hybride combinant des connaissances linguistiques (POS tagging, *Class Splitting*) et des techniques statistiques de Machine Learning pour le Bio-NER[cite: 169].

L'importance du **Feature Engineering** (Word Cache, HMM) et du **d√©codage de Viterbi** est confirm√©e pour garantir la fiabilit√© des informations extraites[cite: 170, 171].

### Prochaines √âtapes

Le travail pourrait √™tre √©tendu en utilisant des mod√®les de *Deep Learning* contextuels comme **BioBERT** afin de mieux capturer les d√©pendances √† plus longue port√©e[cite: 172].

---
## üßë‚Äçüíª Auteurs du Projet

* Youssef RAHLI [cite: 14]
* Zouitni SALAH EDDINE [cite: 15]
* Yassir CHERGUI [cite: 16]

**Supervision** : Pr. ED-DRISSIYA EL-ALLALY [cite: 18, 19]

**Ann√©e Acad√©mique** : 2025-2026 [cite: 17]
**Module** : NLP (Natural Language Processing) [cite: 93, 103]
